			+--------------------+
			|        CS 140      |
			| PROJECT 1: THREADS |
			|   DESIGN DOCUMENT  |
			+--------------------+
				   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Tongda Zhang  <tdzhang@stanford.edu>
Bing Han  <bhan11@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

			     ALARM CLOCK
			     ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

-- changes in timer.c--
  ->add a static variable
  static struct list sleep_list;   /* List of all sleeping threads waiting
   for waking up */

--changes in thread.h--
  ->add to struct thread
  int64_t wakeup_ticks;     /* Time ticks’ number to wake up current thread */
  struct list_elem sleep_elem;        /* List element for thread sleep_list */



---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

Answer: In timer_sleep(), it first calculates the the wake up time point 
(wakeup_ticks) for the current thread and insert the thread’s sleep_elem into
the sleep_list in order. The order is decided by wakeup time of each thread 
from the earliest to the latest. Then it puts the current thread to sleep by 
blocking it and the thread_block() will internally pick up the next ready 
thread or idle thread to run. The blocking step can avoid the busy waiting 
problem which exist before. 

In the timer interrupt handler, it selects from the front of the ordered 
sleep_list and then removes and unblocks the threads whose wakeup time point
has already been or passed the current time point. Because the elements in
the sleep_list are in order of wakeup_ticks(the time point to wake up), we 
only need to check the threads from the front of the list. In this way these 
threads are woke up and put back to ready_list by thread_unblock() and ready 
to run.


>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

Answer: The sleep_list is ordered by threads’ wakeup time from the earliest
to the latest. The ordering is done during insertion by list_insert_ordered()
with a comparing function time_compare_less() we defined. So when trying to 
pick up the threads to wake up in the timer interrupt handler, instead of 
checking all the threads in the list, it only needs to pull from the front of
the sleep_list to get one or several threads that are ready to wake up instead
of traversing the whole list. In this way we minimize the amount of time spent
in the timer interrupt handler.

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

Answer: The race conditions are avoided by disabling interrupt in 
timer_sleep(). So even if multiple threads call timer_sleep() simultaneously,
only one of them could get the position and complete the set of operation 
involving sleep_list and thread_block() atomically without interrupted by 
all other threads.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

Answer: The race conditions are avoided by disabling interrupt in 
timer_sleep(). In the timer interrupt handler, it will read from the 
front of the shared 
variable, sleep_list, and may remove elements from it as well. With 
interruption disabled in timer_sleep(), it will atomically complete the
modification of the shared variable, sleep_list, and avoid race conditions
due to timer interrupt.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

Answer:
To achieve this functionality, there are many possible ways. One way we
thought about is to add a boolean flag variable in the thread structure
to indicate whether the thread is in sleep mode or not, and an integer 
wakeup_ticks variable to indicate the time tick point that the thread 
needs to wake up. Under this design, when a thread called thread_sleep(),
the flag variable is set to 1 indicating its ‘sleep mode’, and the 
wakeup_ticks is updated accordingly. Then in the timer interrupt handler,
we need to go through the whole thread list (all_list) to find the threads
with flag equals true and wakeup_ticks is less or equal to current time 
ticks to wake them up.

The above approach, compared to the design we used in the project, uses a 
flag variable instead a global list ‘sleep_list’ to keep track of all the 
sleep threads. It avoided additional global list which reduces the difficulty
to maintain that additional global list. However, the complexity to 
find/search and wake up the right sleeping threads in the timer interrupt 
handler is much higher than the design we used. It needs to search all the 
threads to find the those that need to be waked up, which is with complexity 
of O(n) where n is the number of all the threads.  The design we used only 
is of O(k) (k is the number of threads that need to be wake up) to find and 
wake up corresponding threads since the sleep_list is in order and only the 
k threads in the front need to be checked.

			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.


-- changes in synch.h --
  ->add to struct lock
  struct list_elem lock_elem;  /* List element for lock list */


-- changes in thread.h --
  ->add to struct thread
  int actual_priority;     /* actual_priority with the effect of donation.*/
  struct lock *wanted_lock;    /* the lock this thread is waiting for */
  struct list waited_by_other_lock_list;  /* the list of locks that this 
  thread is currently holding while waited by some other threads */

-- changes in thread.c --
  -->changes in static variable
  static struct list ready_list[PRI_MAX+1];   /*64 ready queues/lists, each
  queue/list refers to one priority value */

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

Answer: 
The variable “actual_priority” is used to keep track of a thread’s actual 
priority with all the donations. It separates the priority with donations 
and the original/generic priority. So the donation will not override a 
thread’s original/generic priority. Therefore, a thread can return to its 
generic priority once the lock is released and corresponding donation is 
revoked. 

The donation process begins when a thread tries to acquire a lock that is
held by another thread and its actual priority is larger than that of the
lock holder. The thread will be added to the waiters list (inside the
semaphore structure which is inside the lock structure) of the lock, and
it will get the lock holder (through wanted_lock and then the holder) and
try to donate its actual priority if its actual_priority is higher than
the holder’s. The donation will recursively happen if the holder is also
waiting for a lock and its new actual_priority is higher than that lock’s
holder. It will then donate its new actual_priority to that lock’s holder.

The following diagram shows the nested priority donation situation. A 
lock_acquire action from thread C to B triggered a priority donation 
from C to B, and then B to A. Because Priority(C)>Priority(B)>Priority(A) 
and C wants to acquire a lock held by B, and B wants to acquire a lock held
by A. Therefore, the actual priority of A and B will be bumped up to HIGH 
because of the priority donation chain starts from thread C.

    ***************
    *  Thread A   *
    *Priority LOW *
    ***************
          ^         \
          |          \hold
          |           \            
          |            ********
          |Donate      *Lock 1*
          |            ********
          |           /
          |          /wait
          |         / 
    ***************
    *  Thread B   *
    *Priority MED *
    ***************
          ^         \
          |          \hold
          |           \
          |            ********
          |Donate      *Lock 2*
          |            ********
          |           /
          |          /wait
          |         /    
    ***************
    *  Thread C   *
    *Priority HIGH*
    ***************

There is another donation situation known as multiple priority donation.
When one lock is waited by multiple threads, those threads will all try
to donate their priorities. The lock holder will then be donated with 
highest priority among them, and set its actual_priority to it if its 
own generic priority is lower than that.

To track the priority donaton, we also need to handle how the donation 
will be revoked when a lock is released. If the lock-releasing thread 
holds only one lock, it will set its actual_priority back to its generic 
priority when the lock is released. If the thread holds more than one 
locks, instead of setting the actual_priority back to its generic 
priority, it needs to go through all the remaining locks, find the 
highest donation priority from threads in those locks’ waiters list 
(inside the semaphore structure which is inside the lock structure). 
It will set its actual priority to the highest donation priority if 
its own generic priority is lower. 




---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

Answer: 
For a lock or semaphore, when releasing the lock or sema_up the semaphore,
we will find the thread with maximum actual priority (using round-robin if
multiple threads with the same actual priority) from the semaphore’s 
waiters and unblock this thread. If this thread’s actual priority is 
larger than that of the current thread, we will call thread_yield() 
which internally schedules to run the thread with the highest priority
(using round-robin if multiple threads with the same actual priority).

For condition variable, when cond_signal() is called, we will find the 
semaphore waited by the maximum actual priority thread from the condition 
variable’s waiter list. Then sema_up this semaphore. As described above,
the call of sema_up() will find and unblock the maximum actual priority 
thread and schedule the highest actual priority thread to run (using 
round-robin if multiple threads with the same actual priority) if the 
actual priority is larger than that of the current thread.


>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

Answer:
When lock_acquire() is called, it first disables interrupt to avoid race
conditions. Then:
1) if it’s not in mlfqs mode and the lock is currently hold by another 
thread, following three events happen: (suppose thread A is the current 
thread which is acquiring the lock and thread B is the current holder of
the lock)
  i) set A’s wanted_lock to the lock
 ii) add the lock to B’s waited_by_other_lock_list (since the lock is at 
 least waited by A) if the list does not have the lock
 iii) A sets B’s actual priority to A’s actual priority (donation) if A’s
 actual priority is larger than B’s.
2) sema_down the lock’s semaphore, it will block A if the sema_down is 
blocked (which means the lock is held by another thread)
3) set lock holder to A (when the above sema_down returns, the current 
thread is going to be the new holder)
4) if not in mlfqs mode, then
  i) set A’s wanted_lock to NULL, since A has already got the lock.
 ii) if the lock is still waited by others, add the lock into A’s 
 waited_by_other_lock_list 

We handle the nested donation in thread_set_actual_priority(). In 
thread_set_actual_priority(), it set B’s actual priority to the new 
priority. Then if it’s not in mlfqs mode and B’s wanted_lock is not NULL,
the nested donation is triggered. B will find its wanted_lock’s holder, 
thread C, and set C’s actual priority to the new priority passed to B if 
C’s actual priority is smaller than this new priority by calling 
thread_set_actual_priority() recursively. Since the ready_list consists 
of 64 ready thread queues (each queue stored the threads with the same 
actual priority), we also need to move thread B from its previous queue 
to the new queue due to its actual priority change.




>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

Answer:
When lock_release() is called, it first disable interrupt to avoid race 
conditions. (suppose thread A is the thread that is going to release the 
lock)
1) if not in mlfqs mode
  i) remove the lock from A’s waited_by_other_lock_list if the lock is 
within its waited_by_other_lock_list
 ii) find the maximum actual priority in A’s waited_by_other_lock_list.
 If this maximum actual priority is larger than A’s priority, then set 
 A’s actual priority to this maximum actual priority. Otherwise, set A’s
 actual priority to its own priority.
2) set the lock holder to NULL
3) sema_up the lock. In sema_up, if sema’s waiters list is not empty, we
find the thread M with maximum actual priority, then remove M from the 
waiters list and unblock M. If M’s actual priority is larger than that 
of the current thread, call thread yield().



---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

Answer: 
The thread_set_priority(new_priority) sets the priority of a thread to 
new_priority, and then tries to update its actual_priority. It will first 
go through all the locks held by the thread and then find the highest 
donation actual_priority from locks’ waiter threads. Then it compares that
highest donation actual_priority with the thread’s own generic priority. 
It will use thread_set_actual_priority to set its actual_priority to the 
bigger value.

A potential race will happen when the thread A is just about to update its
actual_priority from others’ donation or its new_priority, a thread B with
higher priority occupies the cpu and donates its higher priority to thread
A. After B releases the cpu, thread A resumes running, it will execute the
actual_priority update which will override the new higher value that B just
donates. It is incorrect. So we need to disable the interrupt to avoid the
potential race situation.

Using a lock to avoid such situation will not work. Using a lock will lead
thread B to donate priority to A again, then just as described above, when 
B releases the cpu, thread A resumes running, A will be set to the 
previously calculated priority value instread of the new higher B’s actual 
priority, which is incorrect.


---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

Answer:
There are other ways to meet the functionality requirement. One way is to 
add a global all_lock_list instead of a waited_by_other_lock_list in thread
struct. Under this design, it costs more space to store the global 
all_lock_list but saves space for the waited_by_other_lock_list of each 
thread. The substantial disadvantage is that every time a thread reads 
(for example, find the max actual priority thread blocked by itself) the 
all_lock_list, it has to compare itself with all the locks’ holder to 
collect all the locks it is holding, then execute the operations based 
on this. This traverse is of complexity of O(n) where n is the number of
locks globally, which is not good due to the large amount of time it costs.

Another thing we chose to do is to modify the ready_list from a single list
to an array of lists, and each list stores the threads with the same actual
priority. We did this because firstly it is compatible with the following
advanced scheduler (mlfqs mode). Secondly, every time picking the highest
priority thread to run, we just need to read from the highest non-empty 
priority list and pick up the first one (round-robin fashion), and every 
time inserting thread into ready_list we just need to push it back 
(round-robin fashion) to its corresponding list, and every time a thread’s 
actual priority changes, we just need to remove the thread from its previous 
list and push it back to its new corresponding list, so we avoid a whole list 
sort due to thread’s actual priority change in the single list scenario.



			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

--changes in thread.h--
#define NICE_MAX 20   /*the highest nice value*/
#define NICE_MIN -20  /*the lowest nice value*/  
->add to struct thread
  int recent_cpu;     /*fixed-point recent_cpu value used for -mlfqs*/
  int nice;                /*nice value used for -mlfqs*/

--changes in thread.c--
  ->add static variable
   static int32_t load_avg; /*fixed_point, load_avg, the value of load
   average for -mlfqs use*/

--add file fixed_point.c/h--
/*defined a serial of operations of fixed point number*/

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0  0  0  63 61 59    A    (Highest Priority)
 4      4  0  0  62 61 59    A    (Highest Priority)
 8      8  0  0  61 61 59    B    (Round-Robin)
12      8  4  0  61 60 59    A    (Highest Priority)
16     12  4  0  60 60 59    B    (Round-Robin)
20     12  8  0  60 59 59    A    (Highest Priority)
24     16  8  0  59 59 59    C    (Round-Robin)
28     16  8  4  59 59 58    B    (Round-Robin)
32     16  12 4  59 58 58    A    (Highest Priority)
36     20  12 4  58 58 58    C    (Round-Robin)

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

Answer: Yes. At several time points (tick 8, 16, 24, 28 and 36), there are
more than one threads are with the highest priority. To choose which thread
to run from those with the highest priority, certain rules need to apply 
to avoid ambiguities. 

In the table above, we use first come first serve rule for the round-robin 
fashion. Therefore, among all the threads with the highest priority, we 
choose the one waited in that priority queue for the longest time.

This schedule strategy is also used by our scheduler.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

Answer: The more work you put inside the interrupt context, the longer it 
will take to process interrupt. And it will result in longer waiting time 
for other waiting threads and thus a lower performance to some extent 
because no concurrency can happen inside the interrupt context. 

Therefore, what we did is to put as less work as we can inside the 
interrupt context. Only statistics and priority updates will happen 
inside the interrupt context, and the reschedule work is triggered if 
and only if the new updated priority of some ready thread is higher 
than the current thread’s.

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

Answer:
Advantages:
1) The ready_list is an array of lists, each of which stores the threads 
with the same actual_priority. So when picking the highest priority 
thread to run, we just need to read from the highest non-empty priority 
list and pick up the first one (FCFS, round-robin fashion), and when 
inserting thread into the ready_list we just need to push it back (FCFS, 
round-robin fashion) to its corresponding list, and when a thread’s 
priority changes, we only need to remove the thread from its previous 
list and push it back to its new corresponding list, so we avoid a whole 
list sort due to thread’s priority change in the single list scenario.

2) From the design point of view, we analyze the functionalities of original
scheduler in previous part and this advanced one and find the the logics 
which they can share and which they are mutual exclusive. Then for those 
logics which they are mutually exclusive, we use the flag thread_mlfqs to 
switch between the two schedulers, and for those they can share, we use the
same implementation. In this way we can reduce the code change thus reduce
the chance of error and facilitate testing by switch between the two modes.

Disadvantages:
1) We could add a counter variable to keep track of the total number of 
threads in ready_list. This counter is incremented or decremented every 
time a thread is changed to THREAD_READY or changed from THREAD_READY. 
In this way we do not have to sum up the length of all the lists to get
the number of ready threads, which will decrease the time of updating 
load_avg every second.

Given more time, we will work on the counter mentioned above.


>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

Answer: During the implementation of the advanced scheduler, we find that
the operation of updating threads’ recent_cpu, priority and load_avg 
includes a lot of calculation between fixed-point and fixed-point and 
between fixed-point and integer. So we created new files “fixed_point.c/h”
which include all the math operations (addition, subtraction, multiply, 
division, conversion) among fixed-point numbers themselves and 
fixed-point integer mixed situation. We use inline modifier to implement
all the functionalities about fixed-point, since all these functions 
will be called frequently to update the statistics and priority values, 
and inline function may make the compiler to do some form of extra effort 
to call the function faster (generally by substituting the code of the 
function into its caller).


			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

Answer: It took a lot of time to finish it even when we had already 
figured out all the concepts and procedures. Since it is the first 
assignment, we struggled at the beginning on how to divide the tasks 
and which is the best way to debug. After some back and forth, it’s 
become manageable. In a nutshell, this assignment is challenging but 
interesting.

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

Answer: We feel every part of the project gives us a better understanding 
of the corresponding OS design.

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

Answer: All the materials about the project are really in great detail.

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?
Answer: No.

>> Any other comments?
Answer: No
